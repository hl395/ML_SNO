# -*- coding: utf-8 -*-
"""cnn_bilstm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hP-FJtiXRDGycOzI5Ul1gzQJwoMnQ9WF
"""
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
#from __future__ import absolute_import, division, print_function, unicode_literals
#!pip install -q tensorflow-gpu

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

#!cat /proc/meminfo

#!cat /proc/cpuinfo


import re
import smart_open
import random
import gensim
from pprint import pprint
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
from sklearn.utils import shuffle
import csv
import tokenization
import tensorflow as tf
import matplotlib
from gensim.models import KeyedVectors



class InputExample(object):
  """A single training/test example for simple sequence classification."""

  def __init__(self, guid, id_a, id_b, text_a, text_b, label=None):
    """Constructs a InputExample.

    Args:
      guid: Unique id for the example.
      text_a: string. The untokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The untokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train and dev examples, but not for test examples.
    """
    self.guid = guid
    self.id_a = id_a
    self.id_b = id_b
    self.text_a = text_a
    self.text_b = text_b
    self.label = label

class DataProcessor(object):
  """Base class for data converters for sequence classification data sets."""

  def get_train_examples(self, data_dir):
    """Gets a collection of `InputExample`s for the train set."""
    raise NotImplementedError()

  def get_dev_examples(self, data_dir):
    """Gets a collection of `InputExample`s for the dev set."""
    raise NotImplementedError()

  def get_test_examples(self, data_dir):
    """Gets a collection of `InputExample`s for prediction."""
    raise NotImplementedError()

  def get_labels(self):
    """Gets the list of labels for this data set."""
    raise NotImplementedError()

  @classmethod
  def _read_tsv(cls, input_file, quotechar=None):
    """Reads a tab separated value file."""
    with tf.gfile.Open(input_file, "r") as f:
      reader = csv.reader(f, delimiter="\t", quotechar=quotechar)
      lines = []
      for line in reader:
        lines.append(line)
      return lines

class CNNProcessor(DataProcessor):
  """Processor for the MRPC data set (GLUE version)."""
  def _read_tsv(cls, input_file, quotechar=None):
    """Reads a tab separated value file."""
    with tf.gfile.Open(input_file, "r") as f:
      reader = csv.reader(f, delimiter="\t", quotechar=quotechar)
      lines = []
      for line in reader:
        lines.append(line)
      return lines

  def get_train_examples(self, data_dir, hier, tax):
    """See base class."""
    file_name = hier + "_" + tax + "_"
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, file_name+"train.tsv")), "train")

  def get_dev_examples(self, data_dir, hier, tax):
    """See base class."""
    file_name = hier + "_" + tax + "_"
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, file_name+"dev.tsv")), "dev")

  def get_test_examples(self, data_dir, hier, tax):
    """See base class."""
    file_name = hier + "_" + tax + "_"
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, file_name+"test.tsv")), "test")

  def get_labels(self):
    """See base class."""
    return ["0", "1"]

  def _create_examples(self, lines, set_type):
    """Creates examples for the training and dev sets."""
    examples = []
    for (i, line) in enumerate(lines):
      if i == 0:
        continue
      guid = "%s-%s" % (set_type, i)
      id_a = int(tokenization.convert_to_unicode(line[1]))
      id_b = int(tokenization.convert_to_unicode(line[2]))
      text_a = tokenization.convert_to_unicode(line[3])
      text_b = tokenization.convert_to_unicode(line[4])
      if set_type == "test":
        label = int(tokenization.convert_to_unicode(line[-1]))
      else:
        label = int(tokenization.convert_to_unicode(line[0]))
      examples.append(
          InputExample(guid=guid, id_a=id_a, id_b=id_b, text_a=text_a, text_b=text_b, label=label))
    return examples


def get_batches_infinite(train_examples, embed_list, batch_size=64, random_flag=True):
    samples = train_examples
    num_samples = len(samples)
    if random_flag:
        shuffle(samples)
    while True:
        for offset in range(0, num_samples, batch_size):
            batch_samples = samples[offset:offset + batch_size]
            X_samples = []
            Y_samples = []
            for batch_sample in batch_samples:
                id_pair_list = [batch_sample.id_a, batch_sample.id_b]
                vector_tuple = ()
                for embed in embed_list:
                    a_vector = get_vector_from_elmo(embed, batch_sample.id_a)
                    b_vector = get_vector_from_elmo(embed, batch_sample.id_b)
                    vector_tuple = vector_tuple + (a_vector, b_vector)
                # c_vector = np.concatenate((a_vector_2, b_vector_2), axis=0)
                data_vector = np.array(vector_tuple).T
                X_samples.append(data_vector)
                class_label = batch_sample.label
                Y_samples.append(class_label)

            X_samples = np.array(X_samples).astype('float32')
    #         Y_samples = np.eye(n_classes)[Y_samples]
            #             print('one batch ready')
            assert len(X_samples) == len(Y_samples)
            if random_flag:
                yield shuffle(X_samples, Y_samples)
            else:
                yield (X_samples, Y_samples)


def get_batches_one_iter(train_examples, embed_list, batch_size=64, random_flag=True):
    samples = train_examples
    num_samples = len(samples)
    if random_flag:
        shuffle(samples)
    for offset in range(0, num_samples, batch_size):
        batch_samples = samples[offset:offset + batch_size]
        X_samples = []
        Y_samples = []
        for batch_sample in batch_samples:
            id_pair_list = [batch_sample.id_a, batch_sample.id_b]
            vector_tuple = ()
            for embed in embed_list:
                a_vector = get_vector_from_elmo(embed, batch_sample.id_a)
                b_vector = get_vector_from_elmo(embed, batch_sample.id_b)
                vector_tuple = vector_tuple + (a_vector, b_vector)
            # c_vector = np.concatenate((a_vector_2, b_vector_2), axis=0)
            data_vector = np.array(vector_tuple).T
            X_samples.append(data_vector)
            class_label = batch_sample.label
            Y_samples.append(class_label)

        X_samples = np.array(X_samples).astype('float32')
#         Y_samples = np.eye(n_classes)[Y_samples]
        #             print('one batch ready')
        assert len(X_samples) == len(Y_samples)
        if random_flag:
            yield shuffle(X_samples, Y_samples)
        else:
            yield (X_samples, Y_samples)


def get_vector_from_elmo(embed, key, length=512):
    if key in embed:
        result = embed[key]
        result = np.pad(result, (0, length-len(result)), 'constant')
        return result
    else:
        print("embeddings for concept id {} does not exist".format(key))

def l2_norm(x):
   return np.sqrt(np.sum(x**2))

def div_norm(x):
   norm_value = l2_norm(x)
   if norm_value > 0:
       return x * (1.0 / norm_value)
   else:
       return x


def bioclean(str):
    # clean for BioASQ
    tokens = re.sub('[.,?;*!%^&_+():-\[\]{}]', '',
                                str.replace('"', '').replace('/', '').replace('\\', '').replace("'",
                                                                                              '').strip().lower()).split()
    return tokens

def fetch_sentence_vector(model, str, tokenizer, length = 200):
    tokens = bioclean(str)
    vec_list = []
    not_exist_list = []
    for to in tokens:
        try:
            vec_list.append(model[to])
        except:
            inner_vec_list = []
            word_list = tokenizer.tokenize(to)
            for word in word_list:
                word = word.replace("##", "")
                try:
                    inner_vec_list.append(model[word])
                except:
                    not_exist_list.append(word)
                    print('{} not exists'.format(word))
                    inner_vec_list.append(np.zeros(length, dtype=float))
            inner_vec_list = np.average(inner_vec_list, axis=0)
            # print(inner_vec_list)
            vec_list.append(inner_vec_list)
    vec_list = [div_norm(x) for x in vec_list]
    return np.average(vec_list, axis=0)


def get_multi_embeddings_from_tfhub(examples, embedding_type_list, batch_size=500):
    # Create graph and finalize (optional but recommended).
    concept_dict = {}
    result_embedding_list =[]
    for example in examples:
        if example.id_a not in concept_dict:
            concept_dict[example.id_a] = example.text_a
        if example.id_b not in concept_dict:
            concept_dict[example.id_b] = example.text_b

    id_list = []
    text_list = []
    for i, text in concept_dict.items():
        id_list.append(i)
        text_list.append(text)

    # id_list = id_list[:500]
    # text_list = text_list[:500]
    # print(len(text_list))
    # print(text_list[1090:1095])

    if "elmo" in embedding_type_list:
        print("get embeddings from source: elmo dim:1024")

        g = tf.Graph()
        with g.as_default():
            text_input = tf.placeholder(dtype=tf.string, shape=[None])
            embed = hub.Module("https://tfhub.dev/google/elmo/2", trainable=True)
            my_result = embed(text_input)
            init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
        g.finalize()

        # Create session and initialize.
        # config = tf.ConfigProto(
        #     device_count={'GPU': 0}
        # )
        # session = tf.Session(graph=g, config=config)
        session = tf.Session(graph=g)
        session.run(init_op)
        # Loop over batches
        vector_result = []

        for batch in get_batches_examples_for_elmo(text_list, batch_size=batch_size):
            # Feed dictionary
            re_x = session.run(my_result, feed_dict={text_input: batch})
            vector_result.extend(re_x)
        assert len(id_list) == len(vector_result)
        session.close()
        emlo_embeddings = dict(zip(id_list, vector_result))
        result_embedding_list.append(emlo_embeddings)

    if "usel" in embedding_type_list:
        print("get embeddings from source: universal-sentence-encoder-large v3")

        # tf.reset_default_graph()
        g2 = tf.Graph()
        with g2.as_default():
            text_input_2 = tf.placeholder(dtype=tf.string, shape=[None])
            embed2 = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-large/3", trainable=True)
            my_result_2 = embed2(text_input_2)
            init_op_2 = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
        g2.finalize()

        session_2 = tf.Session(graph=g2)
        session_2.run(init_op_2)
        vector_result_2 = []
        count = 0
        for batch in get_batches_examples_for_elmo(text_list, batch_size=batch_size):
            # Feed dictionary
            count += 1
            # print("working on batch {}".format(count))
            re_x_2 = session_2.run(my_result_2, feed_dict={text_input_2: batch})
            vector_result_2.extend(re_x_2)
        assert len(id_list) == len(vector_result_2)

        session_2.close()
        usel_embeddings = dict(zip(id_list, vector_result_2))
        result_embedding_list.append(usel_embeddings)

    if "wiki500norm" in embedding_type_list:
        print("get embeddings from source: Wiki-words-500-with-normalization")

        # tf.reset_default_graph()
        g3 = tf.Graph()
        with g3.as_default():
            text_input_3 = tf.placeholder(dtype=tf.string, shape=[None])
            embed3 = hub.Module("https://tfhub.dev/google/Wiki-words-500-with-normalization/1", trainable=True)
            my_result_3 = embed3(text_input_3)
            init_op_3 = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
        g3.finalize()

        session_3 = tf.Session(graph=g3)
        session_3.run(init_op_3)
        vector_result_3 = []
        count = 0
        for batch in get_batches_examples_for_elmo(text_list, batch_size=batch_size):
            # Feed dictionary
            count += 1
            # print("working on batch {}".format(count))
            re_x_3 = session_3.run(my_result_3, feed_dict={text_input_3: batch})
            vector_result_3.extend(re_x_3)
        assert len(id_list) == len(vector_result_3)

        session_3.close()
        wiki500_norm_embeddings = dict(zip(id_list, vector_result_3))
        result_embedding_list.append(wiki500_norm_embeddings)

    if "biovec" in embedding_type_list:
        print("get embeddings from source: biovec size:200")
        tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)
        biovec_model = KeyedVectors.load_word2vec_format(FLAGS.biovec_embeddings_file, binary=True)
        biovec_result = []
        for i, concept_id in enumerate(id_list):
            test_vec = fetch_sentence_vector(biovec_model, concept_dict[concept_id], tokenizer, 200)
            biovec_result.append(test_vec)
            if i % 1000 == 0:
                print("Loading progress: {:.2f}%".format(i/len(id_list)*100))
        assert len(id_list) == len(biovec_result)

        biovec_embeddings = dict(zip(id_list, biovec_result))
        result_embedding_list.append(biovec_embeddings)

    if "pubmed" in embedding_type_list:
        print("get embeddings from source: pubmed word2vec size:400")
        tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)
        pubmed_model = KeyedVectors.load_word2vec_format(FLAGS.pubmed_embeddings_file, binary=True)
        pubmed_result = []
        for i, concept_id in enumerate(id_list):
            test_vec = fetch_sentence_vector(pubmed_model, concept_dict[concept_id], tokenizer, 400)
            pubmed_result.append(test_vec)
            if i % 1000 == 0:
                print("Loading progress: {:.2f}%".format(i/len(id_list)*100))
        assert len(id_list) == len(pubmed_result)

        pubmed_embeddings = dict(zip(id_list, pubmed_result))
        result_embedding_list.append(pubmed_embeddings)

    if "fasttext" in embedding_type_list:
        print("get embeddings from source: fasttext word2vec size:300")
        from gensim.models import FastText
        fasttext_model = FastText.load_fasttext_format(FLAGS.fasttext_model_file)
        fasttext_result = []
        for i, concept_id in enumerate(id_list):
            test_vec = fetch_sentence_vector(fasttext_model, concept_dict[concept_id], 300)
            fasttext_result.append(test_vec)
            if i % 1000 == 0:
                print("Loading progress: {:.2f}%".format(i/len(id_list)*100))
        assert len(id_list) == len(fasttext_result)

        fasttext_embeddings = dict(zip(id_list, fasttext_result))
        result_embedding_list.append(fasttext_embeddings)

    if "doc2vec_pvdbow" in embedding_type_list:
        print("get embeddings from source: doc2vec_pvdbow")
        tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)
        #  PV-DBOW
        vector_model_file_0 = FLAGS.vector_model_path + FLAGS.hier_name +"_concept_model0"
        pvdbow_model = gensim.models.Doc2Vec.load(vector_model_file_0)

        pvdbow_vector_result = []
        for i, concept_id in enumerate(id_list):
            if concept_id in pvdbow_model.docvecs:
                concept_vector = pvdbow_model.docvecs[concept_id]
            else:
                concept_vector = pvdbow_model.infer_vector(tokenizer.tokenize(concept_dict[concept_id]))
            pvdbow_vector_result.append(concept_vector)
            if i % 1000 == 0:
                print("Loading progress: {:.2f}%".format(i/len(id_list)*100))
        assert len(id_list) == len(pvdbow_vector_result)

        doc2vec_pvdbow_embeddings = dict(zip(id_list, pvdbow_vector_result))
        result_embedding_list.append(doc2vec_pvdbow_embeddings)

    if "doc2vec_pvdm" in embedding_type_list:
        print("get embeddings from source: doc2vec_pvdm")
        tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)
        # PV-DM seems better??
        vector_model_file_1 = FLAGS.vector_model_path + FLAGS.hier_name +"_concept_model1"
        pvdm_model = gensim.models.Doc2Vec.load(vector_model_file_1)
        pvdm_vector_result = []
        for i, concept_id in enumerate(id_list):
            if concept_id in pvdm_model.docvecs:
                concept_vector = pvdm_model.docvecs[concept_id]
            else:
                concept_vector = pvdm_model.infer_vector(tokenizer.tokenize(concept_dict[concept_id]))
            pvdm_vector_result.append(concept_vector)
            if i % 1000 == 0:
                print("Loading progress: {:.2f}%".format(i/len(id_list)*100))
        assert len(id_list) == len(pvdm_vector_result)

        doc2vec_pvdm_embeddings = dict(zip(id_list, pvdm_vector_result))
        result_embedding_list.append(doc2vec_pvdm_embeddings)
    # print(len(vector_result))
    # print(vector_result[1090:1095])
    return result_embedding_list


def get_batches_examples_for_elmo(examples, batch_size=500, random_flag=False):
    samples = examples
    num_samples = len(samples)
    if random_flag:
        shuffle(samples)
    for offset in range(0, num_samples, batch_size):
        batch_samples = samples[offset:offset + batch_size]
        yield batch_samples


def plot_confusion_matrix(cls_true, cls_pred, img_path, img_name, num_classes=2):
    from sklearn.metrics import confusion_matrix
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    # This is called from print_test_accuracy() below.

    # cls_pred is an array of the predicted class-number for
    # all images in the test-set.

    # Get the true classifications for the test-set.

    # Get the confusion matrix using sklearn.
    cm = confusion_matrix(y_true=cls_true,
                          y_pred=cls_pred)

    # Print the confusion matrix as text.
    print(cm)

    # Plot the confusion matrix as an image.
    plt.matshow(cm)

    # Make various adjustments to the plot.
    plt.colorbar()
    tick_marks = np.arange(num_classes)
    plt.xticks(tick_marks, range(num_classes))
    plt.yticks(tick_marks, range(num_classes))
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig(img_path + img_name +'confusion_matrix.png')
    # Ensure the plot is shown correctly with multiple plots
    # in a single Notebook cell.
    plt.show()


def column(matrix, i):
    return [row[i] for row in matrix]

# coding: utf-8
import time
import tensorflow as tf
from keras import regularizers
import matplotlib
import csv
from datetime import datetime
timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')



flags = tf.flags

FLAGS = flags.FLAGS

## Required parameters
flags.DEFINE_string("data_dir", None, "The input data dir. Should contain the .tsv files (or other data files) "
    "for the task.")
flags.DEFINE_string("vocab_file", None,"The vocabulary file that the BERT model was trained on.")
flags.DEFINE_string("output_dir", None, "The output directory where the model checkpoints will be written.")
flags.DEFINE_bool("do_lower_case", True, "Whether to lower case the input text. Should be True for uncased "
    "models and False for cased models.")
flags.DEFINE_bool("do_train", False, "Whether to run training.")
flags.DEFINE_bool("do_eval", False, "Whether to run eval on the dev set.")
flags.DEFINE_bool("do_predict", False, "Whether to run the model in inference mode on the test set.")
flags.DEFINE_string("directory_path", None, "directory path")
flags.DEFINE_string("vector_type", '2vectors', "how many vectors per concept.")
flags.DEFINE_integer("n_channels", 4, "how many channels to the CNN model.")
flags.DEFINE_integer("batch_size", 4000, "how many samples per batch.")
flags.DEFINE_integer("seq_len", 256, "embedding size for each concept.")
flags.DEFINE_float("learning_rate", 0.0001, "learning rate for the CNN model.")
flags.DEFINE_bool("l2_loss_flag", False, "l2 loss flag.")
flags.DEFINE_float("lambda_loss_amount", 0.001, "l2 loss.")
flags.DEFINE_integer("epochs", 10,  "200  debug ****")
flags.DEFINE_integer("n_classes", 2, "how many channels to the CNN model.")
flags.DEFINE_string("img_path", "img/", "")
flags.DEFINE_bool("one_vector_flag", False, "Whether to run the model in inference mode on the test set.")
flags.DEFINE_integer("iterations", 40, "vector training parameters: 10, 20, 40.")
flags.DEFINE_string("hier_name", None, "hierarchy name: clinical_finding, procedure")
flags.DEFINE_string("taxonomy", "hier", "summarization type: hier, area, parea")
flags.DEFINE_string("vector_model_path", None, "where to get vector embeddings")
flags.DEFINE_string("img_name", "img__" + timestamp, "where to get vector embeddings")
flags.DEFINE_string("task_name", "CNN", "task name")
flags.DEFINE_string("elmo", "elmo", "embedding type elmo")
flags.DEFINE_string("usel", "usel", "embedding type usel")
flags.DEFINE_string("wiki500norm", "wiki500norm", "embedding type wiki500norm")
flags.DEFINE_string("doc2vec_pvdbow", "doc2vec_pvdbow", "embedding type pvdbow")
flags.DEFINE_string("doc2vec_pvdm", "doc2vec_pvdm", "embedding type pvdm")
flags.DEFINE_string("saved_model_name", "my_model.hdf5", "saved model name")
flags.DEFINE_string("saved_model_index", "0", "saved model index")
flags.DEFINE_integer("embed_batch_size", 300,  "embedding batch size 300  debug ****")
flags.DEFINE_string("fasttext_model_file", 'SNO/data/crawl-300d-2M-subword/crawl-300d-2M-subword.bin', "fasttext crawl")
flags.DEFINE_string("biovec_embeddings_file", 'SNO/data/BioWordVec/bio_embedding_extrinsic.bin', "bioWordVector")  # gensim w2v in c
flags.DEFINE_string("pubmed_embeddings_file", 'SNO/data/pubmed2018_w2v_400D/pubmed2018_w2v_400D.bin', "pubmed vector")  # gensim w2v in c
flags.DEFINE_string("fasttext", "fasttext", "fasttext")
flags.DEFINE_string("biovec", "biovec", "biovec")
flags.DEFINE_string("pubmed", "pubmed", "pubmed")


######################################################################################################

def main(_):

    ######################################################################################################


    tf.logging.set_verbosity(tf.logging.INFO)
    print("Run training with hierarchy: {} iteration: {}".format(FLAGS.hier_name, FLAGS.iterations))
    print("Testing with negative taxonomy {}".format(FLAGS.taxonomy))

    processors = {
    "cnn": CNNProcessor
    }
    task_name = FLAGS.task_name.lower()

    if task_name not in processors:
      raise ValueError("Task not found: %s" % (task_name))

    processor = processors[task_name]()
    label_list = processor.get_labels()

    # #  PV-DBOW
    # vector_model_file_0 = FLAGS.vector_model_path + "model0"
    # pvdbow_model = gensim.models.Doc2Vec.load(vector_model_file_0)
    #
    # # PV-DM seems better??
    # vector_model_file_1 = FLAGS.vector_model_path + "model1"
    # pvdm_model = gensim.models.Doc2Vec.load(vector_model_file_1)

    train_examples = None
    if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:
      raise ValueError(
          "At least one of `do_train`, `do_eval` or `do_predict' must be True.")

    examples = []
    if FLAGS.do_train:
        train_examples = processor.get_train_examples(FLAGS.data_dir, FLAGS.hier_name, FLAGS.taxonomy)
        num_actual_train_examples = len(train_examples)
        print("number of {} examples: {}".format("train", num_actual_train_examples))
        examples += train_examples
    if FLAGS.do_eval:
        eval_examples = processor.get_dev_examples(FLAGS.data_dir, FLAGS.hier_name, FLAGS.taxonomy)
        num_actual_eval_examples = len(eval_examples)
        print("number of {} examples: {}".format("eval", num_actual_eval_examples))
        examples += eval_examples
    if FLAGS.do_predict:
        predict_examples = processor.get_test_examples(FLAGS.data_dir, FLAGS.hier_name, FLAGS.taxonomy)
        num_actual_predict_examples = len(predict_examples)
        print("number of {} examples: {}".format("predict", num_actual_predict_examples))
        examples += predict_examples

    # examples = train_examples + eval_examples + predict_examples
    # embedding_types = [FLAGS.wiki500norm]
    # embedding_types = [FLAGS.wiki500norm, FLAGS.usel]
    # embedding_types = [FLAGS.elmo, FLAGS.usel, FLAGS.doc2vec_pvdbow, FLAGS.doc2vec_pvdm]
    # embedding_types = [FLAGS.elmo, FLAGS.usel]
    # embedding_types = [FLAGS.fasttext]
    # embedding_types = [FLAGS.biovec, FLAGS.pubmed]
    embedding_types = [FLAGS.biovec]
    # embedding_types = [FLAGS.pubmed]
    # embedding_types = [FLAGS.doc2vec_pvdbow]
    # embedding_types = [FLAGS.doc2vec_pvdm]
    # embedding_types = [FLAGS.doc2vec_pvdbow, FLAGS.doc2vec_pvdm]
    # embedding_types = [FLAGS.wiki500norm, FLAGS.usel, FLAGS.fasttext, FLAGS.biovec, FLAGS.pubmed]
    embeddings_list = get_multi_embeddings_from_tfhub(examples, embedding_type_list=embedding_types, batch_size=FLAGS.embed_batch_size)

    print("load embeddings done")
    one_value =next(iter(embeddings_list[0].values()))
    print(one_value.shape)
    # for (ex_index, example) in enumerate(train_examples):
    #     print("id_a:{}\ttext_a: {}".format(example.id_a, example.text_a))
    #     print("id_b:{}\ttext_b: {}".format(example.id_b, example.text_b))
    # split samples into training and validation set




    if FLAGS.do_train:
        from keras.models import Sequential
        from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, \
            ConvLSTM2D, BatchNormalization, Dense, LSTM, Bidirectional, GRU, SimpleRNN
        from keras.layers import CuDNNGRU, CuDNNLSTM
        from keras.layers import LeakyReLU
        from keras.regularizers import l1, l1_l2, l2
        from sklearn.model_selection import train_test_split
        train_examples, validation_examples = train_test_split(train_examples, test_size=0.2, shuffle=True)
        #   tf.reset_default_graph()
        #   from keras import backend as K
        #   K.clear_session()

        ###################################### work ############################################
        #   model = tf.keras.Sequential([
        #         tf.keras.layers.Conv1D(filters=256, kernel_size=15, strides=1, padding='same', activation='relu', input_shape=(1024, 3)),
        #         tf.keras.layers.MaxPooling1D(),
        #         tf.keras.layers.Conv1D(filters=128, kernel_size=10, strides=1, padding='same', activation='relu'),
        #         tf.keras.layers.MaxPooling1D(),
        #         tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu'),
        #         tf.keras.layers.MaxPooling1D(),
        #         tf.keras.layers.Flatten(),
        #         tf.keras.layers.Dropout(0.1),
        #         tf.keras.layers.Dense(64, activation='relu'),
        #         tf.keras.layers.BatchNormalization(),
        #         tf.keras.layers.Dense(2, activation='softmax')
        #     ])


        ##########################################################################################
        model0 = Sequential([
            Conv1D(filters=256, kernel_size=24, strides=4, padding='same', activation='relu', input_shape=(FLAGS.seq_len, FLAGS.n_channels), kernel_regularizer=l2(0.1)),
            # BatchNormalization(),
            # LeakyReLU(),
            MaxPooling1D(),
            Dropout(0.2),
            Conv1D(filters=128, kernel_size=16, strides=2, padding='same', activation='relu', kernel_regularizer=l2(0.1)),
            # BatchNormalization(),
            # LeakyReLU(),
            MaxPooling1D(),
            Dropout(0.2),
            Conv1D(filters=64, kernel_size=12, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.1)),
            # BatchNormalization(),
            # LeakyReLU(),
            MaxPooling1D(),
            Dropout(0.2),
            Conv1D(filters=64, kernel_size=8, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.1)),
            # BatchNormalization(),
            # LeakyReLU(),
            MaxPooling1D(),
            Dropout(0.2),
            # Conv1D(filters=64, kernel_size=8, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.1)),
            # BatchNormalization(),
            # LeakyReLU(), 
            # MaxPooling1D(),
            # Dropout(0.2),
            Flatten(),
            # BatchNormalization(),
            Dense(64, activation='relu'),
            Dropout(0.2),
            BatchNormalization(),
            Dense(1, activation='sigmoid')
        ])


        model1 = Sequential([
            Conv1D(filters=256, kernel_size=15, strides=1, padding='same', activation='relu',
                   input_shape=(FLAGS.seq_len, FLAGS.n_channels)),
            MaxPooling1D(),
            Conv1D(filters=128, kernel_size=10, strides=1, padding='same', activation='relu',
                   kernel_regularizer=l2(0.001)),
            MaxPooling1D(),
            #       Conv1D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu'),
            #       MaxPooling1D(),
            Bidirectional(CuDNNLSTM(32, return_sequences=True, recurrent_regularizer=l2(0.01))),
            Flatten(),
            Dropout(0.2),
            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dense(1, activation='sigmoid')
        ])

        model2 = Sequential([
            Bidirectional(
                CuDNNLSTM(64, return_sequences=True, recurrent_regularizer=l2(0.01)),
                input_shape=(FLAGS.seq_len, FLAGS.n_channels)),
            Conv1D(filters=64, kernel_size=12, strides=1, padding='same', activation='relu',
                   input_shape=(FLAGS.seq_len, FLAGS.n_channels)),
            MaxPooling1D(),
            Conv1D(filters=64, kernel_size=8, strides=1, padding='same', activation='relu',
                   kernel_regularizer=l2(0.001)),
            MaxPooling1D(),
            #       Conv1D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu'),
            #       MaxPooling1D(),
            # Bidirectional(CuDNNLSTM(32, return_sequences=True, recurrent_regularizer=l2(0.01))),
            Flatten(),
            Dropout(0.2),
            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dense(1, activation='sigmoid')
        ])

        model3 = Sequential([
            CuDNNLSTM(128, input_shape=(FLAGS.seq_len, FLAGS.n_channels), recurrent_regularizer=l2(0.01), return_sequences=True),
            Dropout(0.2),
            CuDNNLSTM(128, recurrent_regularizer=l2(0.01), return_sequences=False),
            Dropout(0.2),
            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dense(1, activation='sigmoid')
        ])


        model4 = Sequential([
            Bidirectional(
                CuDNNLSTM(64, return_sequences=True, recurrent_regularizer=l2(0.01)), input_shape=(FLAGS.seq_len, FLAGS.n_channels)),
            Bidirectional(
                CuDNNLSTM(64, return_sequences=True, recurrent_regularizer=l2(0.01))),
            Flatten(),
            Dropout(0.2),
            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dense(1, activation='sigmoid')
        ])

        model5 = Sequential([
            CuDNNGRU(128, input_shape=(FLAGS.seq_len, FLAGS.n_channels), recurrent_regularizer=l2(0.01), return_sequences=True),
            Dropout(0.2),
            CuDNNGRU(128, recurrent_regularizer=l2(0.01)),
            Dropout(0.2),
            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dense(1, activation='sigmoid')
        ])

        model6 = Sequential([
            SimpleRNN(128, input_shape=(FLAGS.seq_len, FLAGS.n_channels), return_sequences=True, recurrent_regularizer=l2(0.01)),
            SimpleRNN(128, recurrent_regularizer=l2(0.01)),
            Dropout(0.2),
            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dense(1, activation='sigmoid')
        ])
        
        model7 = Sequential([
            Bidirectional(CuDNNGRU(64, return_sequences=True, recurrent_regularizer=l2(0.01)), input_shape=(FLAGS.seq_len, FLAGS.n_channels)),
            #       Flatten(),
            Bidirectional(CuDNNGRU(64, recurrent_regularizer=l2(0.01))),
            Dropout(0.2),
            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dense(1, activation='sigmoid')
        ])

        # Model is the full model w/o custom layers

        from keras.callbacks import TensorBoard

        tensorboard = TensorBoard(log_dir='logs', histogram_freq=0, write_graph=True, write_images=True)
         
        if (os.path.exists(FLAGS.output_dir) == False):
                os.makedirs(FLAGS.output_dir)               

        models = [model0, model1, model2]
        # models = [model0, model1, model2, model3, model4, model5, model6, model7]
        for i, model in enumerate(models):
            model.compile(optimizer='adam',
                    loss='binary_crossentropy',
                    metrics=['accuracy'])
            from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
            earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')
            mcp_save = ModelCheckpoint(FLAGS.output_dir + 'model' + str(i) + '-{epoch:03d}-{acc:03f}-{val_acc:03f}.hdf5', save_best_only=True, monitor='val_loss', mode='min')
            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, min_delta=1e-4, mode='min')
            
            metrics_names = model.metrics_names
            model.summary()

            training_start_time = time.time()

            history = model.fit_generator(generator=get_batches_infinite(train_examples, embed_list=embeddings_list, batch_size=FLAGS.batch_size),
                                          steps_per_epoch=len(train_examples) // FLAGS.batch_size,
                                          validation_data=get_batches_infinite(validation_examples, embed_list=embeddings_list, batch_size=FLAGS.batch_size),
                                          validation_steps=len(validation_examples) // FLAGS.batch_size,
                                          epochs=FLAGS.epochs,
                                          # use_multiprocessing=True,
                                          callbacks=[tensorboard, earlyStopping, mcp_save, reduce_lr_loss])
            training_duration = time.time() - training_start_time
            print("Model {} total training time: {}".format(i, training_duration))
            print(history.history.keys())
            print("Model {} Train: \t acc: {} \t loss: {}".format(i, history.history['acc'], history.history['loss']))

            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            # summarize history for accuracy
            plt.figure()
            plt.plot(history.history['acc'])
            plt.plot(history.history['val_acc'])
            plt.title('model accuracy')
            plt.ylabel('accuracy')
            plt.xlabel('epoch')
            plt.legend(['train', 'test'], loc='upper left')
            plt.savefig(FLAGS.img_path + FLAGS.img_name + 'training_loss_model{}.png'.format(i))
            plt.show()
            # summarize history for loss
            plt.figure()
            plt.plot(history.history['loss'])
            plt.plot(history.history['val_loss'])
            plt.title('model loss')
            plt.ylabel('loss')
            plt.xlabel('epoch')
            plt.legend(['train', 'test'], loc='upper left')
            plt.savefig(FLAGS.img_path + FLAGS.img_name + 'training_accuracy_model{}.png'.format(i))
            plt.show()

            #if (os.path.exists(FLAGS.output_dir) == False):
            #    os.makedirs(FLAGS.output_dir)
            model.save(FLAGS.output_dir + FLAGS.saved_model_name + str(i) + ".hdf5")

    if FLAGS.do_eval:
        print("Evaluate model with {} exmaples:".format(len(eval_examples)))

        from keras.models import load_model
        for i in range(3):
            model = load_model(FLAGS.output_dir + FLAGS.saved_model_name + str(i) + ".hdf5")
    
            metrics_names = model.metrics_names
            for k, v in enumerate(metrics_names):
                print("key: {}\t value: {}".format(k, v))
            model.summary()
    
            eval_history = model.evaluate_generator(
                get_batches_infinite(eval_examples, embed_list=embeddings_list, batch_size=FLAGS.batch_size),
                steps=len(eval_examples)//FLAGS.batch_size)
    
            print("Evaluate model {}: ".format(i), "acc: {} \t loss: {}".format(eval_history[1], eval_history[0]))

    # predict_history = model.evaluate_generator(
    #     get_batches_elmo_multi(predict_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size),
    #     steps=len(predict_examples) // FLAGS.batch_size)
    #
    # print("Predict: ", "acc: {} \t loss: {}".format(predict_history['acc'], predict_history['loss']))

    if FLAGS.do_predict:
        for i in range(3):
            print("Predict model {}: ".format(i))
            print("Test/Predict model with {} new exmaples:".format(len(predict_examples)))
            from keras.models import load_model
            model = load_model(FLAGS.output_dir + FLAGS.saved_model_name + str(i) + ".hdf5")
    
            metrics_names = model.metrics_names
            for k, v in enumerate(metrics_names):
                print("key: {}\t value: {}".format(k, v))
            model.summary()
    
            predict_labels = []
            predict_prob = []
            true_labels = []
    
            for x, y in get_batches_one_iter(predict_examples, embed_list=embeddings_list, batch_size=FLAGS.batch_size, random_flag=True):
                result = model.predict(x)
                predict_prob.extend(result)
                result = np.where(result > 0.5, 1, 0)
                result = result.tolist()
                result_list = []
                [result_list.extend(x) for x in result]
                true_labels.extend(y)
                predict_labels.extend(result_list)
            #   print("predict: ", "average {}: {:.6f}".format(metrics_names[1], np.mean(predict_labels)))
            from sklearn.metrics import classification_report
            print(true_labels[2:5])
            print(predict_labels[2:5])
            print(classification_report(true_labels, predict_labels))
            from sklearn.metrics import roc_curve, auc
            # send the actual dependent variable classifications for param 1,
            # and the confidences of the true classification for param 2.
            FPR, TPR, _ = roc_curve(true_labels, predict_prob)     
            # Calculate the area under the confidence ROC curve.
            # This area is equated with the probability that the classifier will rank
            # a randomly selected defaulter higher than a randomly selected non-defaulter.
            AUC = auc(FPR, TPR)  
            # What is "good" can dependm but an AUC of 0.7+ is generally regarded as good,
            # and 0.8+ is generally regarded as being excellent
            print("AUC is {}".format(AUC))





if __name__ == "__main__":
    # flags.mark_flag_as_required("data_dir")
    # flags.mark_flag_as_required("vocab_file")
    # flags.mark_flag_as_required("output_dir")

    FLAGS.directory_path = "SNO/"
    FLAGS.iterations = 100
    FLAGS.seq_len = 512
    FLAGS.n_channels = 2  # 2 * embedding list size
    FLAGS.hier_name = "procedure"  # "clinical_finding"
    FLAGS.taxonomy = "hier"
    FLAGS.img_path = FLAGS.directory_path + "img/"
    FLAGS.img_name = timestamp + FLAGS.hier_name + "_" + FLAGS.taxonomy + "_iter_" + str(FLAGS.iterations) + "_"
    FLAGS.vocab_file = "MODEL/small/vocab.txt"
    FLAGS.batch_size = 1000  # 2000 kong
    FLAGS.embed_batch_size = 300   # 1000 on kong
    FLAGS.epochs = 10  # 100  on kong
    # FLAGS.data_dir = "../bert/glue_data/MYTESTING/Iteration_8/"
    FLAGS.data_dir = "SNO/TestData/Iteration_6/"
    FLAGS.learning_rate = 0.0001
    FLAGS.output_dir = FLAGS.directory_path +"cnnModel/" + FLAGS.hier_name + "/multi_models/Iteration_6/" + FLAGS.taxonomy + "/6vec/"
    FLAGS.vector_model_path = FLAGS.directory_path + "vectorModel/" + FLAGS.hier_name + "/" + str(FLAGS.iterations) + "/"
    FLAGS.saved_model_name = "concept_100_6vec_model"
    FLAGS.saved_model_index = "0"
    FLAGS.fasttext_model_file = 'SNO/data/crawl-300d-2M-subword/crawl-300d-2M-subword.bin'
    FLAGS.biovec_embeddings_file = 'SNO/data/BioWordVec/bio_embedding_extrinsic.bin'  # gensim w2v in c
    FLAGS.pubmed_embeddings_file = 'SNO/data/pubmed2018_w2v_400D/pubmed2018_w2v_400D.bin'  # gensim w2v in c
    # FLAGS.init_checkpoint = "tmp/procedure/pretraining_output/model.ckpt-10000"  # load the pre-trained model


    FLAGS.do_train = True  # True
    FLAGS.do_eval = True  # True
    FLAGS.do_predict = True  # False
    FLAGS.task_name = "CNN"

    tf.app.run()