# -*- coding: utf-8 -*-
"""cnn_bilstm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hP-FJtiXRDGycOzI5Ul1gzQJwoMnQ9WF
"""

#from __future__ import absolute_import, division, print_function, unicode_literals
#!pip install -q tensorflow-gpu

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

#!cat /proc/meminfo

#!cat /proc/cpuinfo

import os
import re
import smart_open
import random
import gensim
from pprint import pprint
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
from sklearn.utils import shuffle
import csv
import tokenization
import tensorflow as tf
import matplotlib

class InputExample(object):
  """A single training/test example for simple sequence classification."""

  def __init__(self, guid, id_a, id_b, text_a, text_b, label=None):
    """Constructs a InputExample.

    Args:
      guid: Unique id for the example.
      text_a: string. The untokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The untokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train and dev examples, but not for test examples.
    """
    self.guid = guid
    self.id_a = id_a
    self.id_b = id_b
    self.text_a = text_a
    self.text_b = text_b
    self.label = label

class DataProcessor(object):
  """Base class for data converters for sequence classification data sets."""

  def get_train_examples(self, data_dir):
    """Gets a collection of `InputExample`s for the train set."""
    raise NotImplementedError()

  def get_dev_examples(self, data_dir):
    """Gets a collection of `InputExample`s for the dev set."""
    raise NotImplementedError()

  def get_test_examples(self, data_dir):
    """Gets a collection of `InputExample`s for prediction."""
    raise NotImplementedError()

  def get_labels(self):
    """Gets the list of labels for this data set."""
    raise NotImplementedError()

  @classmethod
  def _read_tsv(cls, input_file, quotechar=None):
    """Reads a tab separated value file."""
    with tf.gfile.Open(input_file, "r") as f:
      reader = csv.reader(f, delimiter="\t", quotechar=quotechar)
      lines = []
      for line in reader:
        lines.append(line)
      return lines

class CNNProcessor(DataProcessor):
  """Processor for the MRPC data set (GLUE version)."""
  def _read_tsv(cls, input_file, quotechar=None):
    """Reads a tab separated value file."""
    with tf.gfile.Open(input_file, "r") as f:
      reader = csv.reader(f, delimiter="\t", quotechar=quotechar)
      lines = []
      for line in reader:
        lines.append(line)
      return lines

  def get_train_examples(self, data_dir, hier, tax):
    """See base class."""
    file_name = hier + "_" + tax + "_"
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, file_name+"train.tsv")), "train")

  def get_dev_examples(self, data_dir, hier, tax):
    """See base class."""
    file_name = hier + "_" + tax + "_"
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, file_name+"dev.tsv")), "dev")

  def get_test_examples(self, data_dir, hier, tax):
    """See base class."""
    file_name = hier + "_" + tax + "_"
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, file_name+"test.tsv")), "test")

  def get_labels(self):
    """See base class."""
    return ["0", "1"]

  def _create_examples(self, lines, set_type):
    """Creates examples for the training and dev sets."""
    examples = []
    for (i, line) in enumerate(lines):
      if i == 0:
        continue
      guid = "%s-%s" % (set_type, i)
      id_a = int(tokenization.convert_to_unicode(line[1]))
      id_b = int(tokenization.convert_to_unicode(line[2]))
      text_a = tokenization.convert_to_unicode(line[3])
      text_b = tokenization.convert_to_unicode(line[4])
      if set_type == "test":
        label = int(tokenization.convert_to_unicode(line[-1]))
      else:
        label = int(tokenization.convert_to_unicode(line[0]))
      examples.append(
          InputExample(guid=guid, id_a=id_a, id_b=id_b, text_a=text_a, text_b=text_b, label=label))
    return examples




def get_batches_elmo(train_examples, embed, batch_size=64, n_classes=2, random_flag=True):
    samples = train_examples
    num_samples = len(samples)
    if random_flag:
        shuffle(samples)
    for offset in range(0, num_samples, batch_size):
        batch_samples = samples[offset:offset + batch_size]
        X_samples = []
        Y_samples = []
        for batch_sample in batch_samples:
            id_pair_list = [batch_sample.id_a, batch_sample.id_b]
            a_vector = get_vector_from_elmo(embed, batch_sample.id_a)
            b_vector = get_vector_from_elmo(embed, batch_sample.id_b)
            data_vector = np.array((a_vector, b_vector)).T
            X_samples.append(data_vector)
            class_label = batch_sample.label
            Y_samples.append(class_label)

        X_samples = np.array(X_samples).astype('float32')
        Y_samples = np.eye(n_classes)[Y_samples]
        #             print('one batch ready')
        assert len(X_samples) == len(Y_samples)
        if random_flag:
            yield shuffle(X_samples, Y_samples)
        else:
            yield (X_samples, Y_samples)

def get_batches_elmo_multi(train_examples, embed, embed_2, batch_size=64, n_classes=2, random_flag=True):
    samples = train_examples
    num_samples = len(samples)
    if random_flag:
        shuffle(samples)
    for offset in range(0, num_samples, batch_size):
        batch_samples = samples[offset:offset + batch_size]
        X_samples = []
        Y_samples = []
        for batch_sample in batch_samples:
            id_pair_list = [batch_sample.id_a, batch_sample.id_b]
            a_vector = get_vector_from_elmo(embed, batch_sample.id_a)
            b_vector = get_vector_from_elmo(embed, batch_sample.id_b)
            a_vector_2 = get_vector_from_elmo(embed_2, batch_sample.id_a)
            b_vector_2 = get_vector_from_elmo(embed_2, batch_sample.id_b)
            c_vector = np.concatenate((a_vector_2, b_vector_2), axis=0)
            data_vector = np.array((a_vector, b_vector, c_vector)).T
            X_samples.append(data_vector)
            class_label = batch_sample.label
            Y_samples.append(class_label)

        X_samples = np.array(X_samples).astype('float32')
#         Y_samples = np.eye(n_classes)[Y_samples]
        #             print('one batch ready')
        assert len(X_samples) == len(Y_samples)
        if random_flag:
            yield shuffle(X_samples, Y_samples)
        else:
            yield (X_samples, Y_samples)

def get_batches_elmo_multi_2(train_examples, embed, embed_2, batch_size=64, random_flag=True):
    samples = train_examples
    num_samples = len(samples)
    if random_flag:
        shuffle(samples)
    while True:
        for offset in range(0, num_samples, batch_size):
            batch_samples = samples[offset:offset + batch_size]
            X_samples = []
            Y_samples = []
            for batch_sample in batch_samples:
                id_pair_list = [batch_sample.id_a, batch_sample.id_b]
                a_vector = get_vector_from_elmo(embed, batch_sample.id_a)
                b_vector = get_vector_from_elmo(embed, batch_sample.id_b)
                a_vector_2 = get_vector_from_elmo(embed_2, batch_sample.id_a)
                b_vector_2 = get_vector_from_elmo(embed_2, batch_sample.id_b)
                c_vector = np.concatenate((a_vector_2, b_vector_2), axis=0)
                data_vector = np.array((a_vector, b_vector, c_vector)).T
                X_samples.append(data_vector)
                class_label = batch_sample.label
                Y_samples.append(class_label)

            X_samples = np.array(X_samples).astype('float32')
    #         Y_samples = np.eye(n_classes)[Y_samples]
            #             print('one batch ready')
            assert len(X_samples) == len(Y_samples)
            if random_flag:
                yield shuffle(X_samples, Y_samples)
            else:
                yield (X_samples, Y_samples)




def get_vector_from_elmo(embed, key):
    if key in embed:
        return embed[key]
    else:
        print("embeddings for concept id {} does not exist".format(key))

def get_embeddings_from_elmo(examples, batch_size=100):
    # Create graph and finalize (optional but recommended).
    concept_dict = {}
    for example in examples:
        if example.id_a not in concept_dict:
            concept_dict[example.id_a] = example.text_a
        if example.id_b not in concept_dict:
            concept_dict[example.id_b] = example.text_b

    id_list = []
    text_list = []
    for i, text in concept_dict.items():
        id_list.append(i)
        text_list.append(text)

    # id_list = id_list[:500]
    # text_list = text_list[:500]

    g = tf.Graph()
    with g.as_default():
        text_input = tf.placeholder(dtype=tf.string, shape=[None])
        embed = hub.Module("https://tfhub.dev/google/elmo/2")
        my_result = embed(text_input)
        init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
    g.finalize()

    # Create session and initialize.
    # config = tf.ConfigProto(
    #     device_count={'GPU': 0}
    # )
    # session = tf.Session(graph=g, config=config)
    session = tf.Session(graph=g)
    session.run(init_op)
    # Loop over batches
    vector_result =[]
    for batch in get_batches_examples_for_elmo(text_list, batch_size=batch_size):
        # Feed dictionary
        # feed = {text_input: x}
        re_x = session.run(my_result, feed_dict={text_input: batch})
        vector_result.extend(re_x)
    assert len(id_list) == len(vector_result)
    session.close()

    return dict(zip(id_list, vector_result))

def get_multi_embeddings_from_elmo(examples, batch_size=500):
    # Create graph and finalize (optional but recommended).
    concept_dict = {}
    for example in examples:
        if example.id_a not in concept_dict:
            concept_dict[example.id_a] = example.text_a
        if example.id_b not in concept_dict:
            concept_dict[example.id_b] = example.text_b

    id_list = []
    text_list = []
    for i, text in concept_dict.items():
        id_list.append(i)
        text_list.append(text)

    # id_list = id_list[:500]
    # text_list = text_list[:500]
    # print(len(text_list))
    # print(text_list[1090:1095])
    print("get embeddings from the first source: elmo")

    g = tf.Graph()
    with g.as_default():
        text_input = tf.placeholder(dtype=tf.string, shape=[None])
        embed = hub.Module("https://tfhub.dev/google/elmo/2")
        my_result = embed(text_input)
        init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
    g.finalize()

    # Create session and initialize.
    # config = tf.ConfigProto(
    #     device_count={'GPU': 0}
    # )
    # session = tf.Session(graph=g, config=config)
    session = tf.Session(graph=g)
    session.run(init_op)
    # Loop over batches
    vector_result =[]

    for batch in get_batches_examples_for_elmo(text_list, batch_size=batch_size):
        # Feed dictionary
        re_x = session.run(my_result, feed_dict={text_input: batch})
        vector_result.extend(re_x)
    assert len(id_list) == len(vector_result)
    session.close()

    print("get embeddings from the second source: universal-sentence-encoder-large v3")

    # tf.reset_default_graph()
    g2 = tf.Graph()
    with g2.as_default():
        text_input_2 = tf.placeholder(dtype=tf.string, shape=[None])
        embed2 = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-large/3")
        my_result_2 = embed2(text_input_2)
        init_op_2 = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
    g2.finalize()

    session_2 = tf.Session(graph=g2)
    session_2.run(init_op_2)
    vector_result_2 = []
    count = 0
    for batch in get_batches_examples_for_elmo(text_list, batch_size=batch_size):
        # Feed dictionary
        count += 1
        # print("working on batch {}".format(count))
        re_x_2 = session_2.run(my_result_2, feed_dict={text_input_2: batch})
        vector_result_2.extend(re_x_2)
    assert len(id_list) == len(vector_result_2)

    session_2.close()
    # print(len(vector_result))
    # print(vector_result[1090:1095])
    return dict(zip(id_list, vector_result)), dict(zip(id_list, vector_result_2))


def get_batches_examples_for_elmo(examples, batch_size=500, random_flag=False):
    samples = examples
    num_samples = len(samples)
    if random_flag:
        shuffle(samples)
    for offset in range(0, num_samples, batch_size):
        batch_samples = samples[offset:offset + batch_size]
        yield batch_samples


def plot_confusion_matrix(cls_true, cls_pred, img_path, img_name, num_classes=2):
    from sklearn.metrics import confusion_matrix
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    # This is called from print_test_accuracy() below.

    # cls_pred is an array of the predicted class-number for
    # all images in the test-set.

    # Get the true classifications for the test-set.

    # Get the confusion matrix using sklearn.
    cm = confusion_matrix(y_true=cls_true,
                          y_pred=cls_pred)

    # Print the confusion matrix as text.
    print(cm)

    # Plot the confusion matrix as an image.
    plt.matshow(cm)

    # Make various adjustments to the plot.
    plt.colorbar()
    tick_marks = np.arange(num_classes)
    plt.xticks(tick_marks, range(num_classes))
    plt.yticks(tick_marks, range(num_classes))
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig(img_path + img_name +'confusion_matrix.png')
    # Ensure the plot is shown correctly with multiple plots
    # in a single Notebook cell.
    plt.show()


def column(matrix, i):
    return [row[i] for row in matrix]

# coding: utf-8
import os
import time
import tensorflow as tf
from keras import regularizers
import matplotlib
import csv
from datetime import datetime
timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

flags = tf.flags

FLAGS = flags.FLAGS

## Required parameters
flags.DEFINE_string("data_dir", None, "The input data dir. Should contain the .tsv files (or other data files) "
    "for the task.")
flags.DEFINE_string("vocab_file", None,"The vocabulary file that the BERT model was trained on.")
flags.DEFINE_string("output_dir", None, "The output directory where the model checkpoints will be written.")
flags.DEFINE_bool("do_lower_case", True, "Whether to lower case the input text. Should be True for uncased "
    "models and False for cased models.")
flags.DEFINE_bool("do_train", False, "Whether to run training.")
flags.DEFINE_bool("do_eval", False, "Whether to run eval on the dev set.")
flags.DEFINE_bool("do_predict", False, "Whether to run the model in inference mode on the test set.")
flags.DEFINE_string("directory_path", None, "directory path")
flags.DEFINE_string("vector_type", '2vectors', "how many vectors per concept.")
flags.DEFINE_integer("n_channels", 4, "how many channels to the CNN model.")
flags.DEFINE_integer("batch_size", 4000, "how many samples per batch.")
flags.DEFINE_integer("seq_len", 256, "embedding size for each concept.")
flags.DEFINE_float("learning_rate", 0.0001, "learning rate for the CNN model.")
flags.DEFINE_bool("l2_loss_flag", False, "l2 loss flag.")
flags.DEFINE_float("lambda_loss_amount", 0.001, "l2 loss.")
flags.DEFINE_integer("epochs", 10,  "200  debug ****")
flags.DEFINE_integer("n_classes", 2, "how many channels to the CNN model.")
flags.DEFINE_string("img_path", "img/", "")
flags.DEFINE_bool("one_vector_flag", False, "Whether to run the model in inference mode on the test set.")
flags.DEFINE_integer("iterations", 40, "vector training parameters: 10, 20, 40.")
flags.DEFINE_string("hier_name", None, "hierarchy name: clinical_finding, procedure")
flags.DEFINE_string("taxonomy", "hier", "summarization type: hier, area, parea")
flags.DEFINE_string("vector_model_path", None, "where to get vector embeddings")
flags.DEFINE_string("img_name", "img__" + timestamp, "where to get vector embeddings")
flags.DEFINE_string("task_name", "CNN", "task name")
######################################################################################################

def get_batches_elmo_multi_single(train_examples, embed, embed_2):
    samples = train_examples
    num_samples = len(samples)
    for batch_sample in samples:
        id_pair_list = [batch_sample.id_a, batch_sample.id_b]
        a_vector = get_vector_from_elmo(embed, batch_sample.id_a)
        b_vector = get_vector_from_elmo(embed, batch_sample.id_b)
        a_vector_2 = get_vector_from_elmo(embed_2, batch_sample.id_a)
        b_vector_2 = get_vector_from_elmo(embed_2, batch_sample.id_b)
        c_vector = np.concatenate((a_vector_2, b_vector_2), axis=0)
        data_vector = np.array((a_vector, b_vector, c_vector)).T
        X_sample = data_vector
        class_label = batch_sample.label
        Y_sample = class_label

        X_sample = np.array(X_sample).astype('float32')
#         Y_samples = np.eye(n_classes)[Y_samples]
        yield (X_sample, Y_sample)

def main(_):

    ######################################################################################################


    tf.logging.set_verbosity(tf.logging.INFO)
    print("Run training with hierarchy: {} iteration: {}".format(FLAGS.hier_name, FLAGS.iterations))
    print("Testing with negative taxonomy {}".format(FLAGS.taxonomy))

    processors = {
    "cnn": CNNProcessor
    }
    task_name = FLAGS.task_name.lower()

    if task_name not in processors:
      raise ValueError("Task not found: %s" % (task_name))

    processor = processors[task_name]()
    label_list = processor.get_labels()

    # #  PV-DBOW
    # vector_model_file_0 = FLAGS.vector_model_path + "model0"
    # pvdbow_model = gensim.models.Doc2Vec.load(vector_model_file_0)
    #
    # # PV-DM seems better??
    # vector_model_file_1 = FLAGS.vector_model_path + "model1"
    # pvdm_model = gensim.models.Doc2Vec.load(vector_model_file_1)

    train_examples = None
    if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:
      raise ValueError(
          "At least one of `do_train`, `do_eval` or `do_predict' must be True.")

    if FLAGS.do_train:
        train_examples = processor.get_train_examples(FLAGS.data_dir, FLAGS.hier_name, FLAGS.taxonomy)
        num_actual_train_examples = len(train_examples)
        print("number of {} examples: {}".format("train", num_actual_train_examples))
    if FLAGS.do_eval:
        eval_examples = processor.get_dev_examples(FLAGS.data_dir, FLAGS.hier_name, FLAGS.taxonomy)
        num_actual_eval_examples = len(eval_examples)
        print("number of {} examples: {}".format("eval", num_actual_eval_examples))
    if FLAGS.do_predict:
        predict_examples = processor.get_test_examples(FLAGS.data_dir, FLAGS.hier_name, FLAGS.taxonomy)
        num_actual_predict_examples = len(predict_examples)
        print("number of {} examples: {}".format("predict", num_actual_predict_examples))

    examples = train_examples + eval_examples + predict_examples
    embedding_result, embedding_result_2 = get_multi_embeddings_from_elmo(examples, 300)

    print("load embeddings done")
    one_value =next(iter(embedding_result.values()))
    print(one_value.shape)
    # for (ex_index, example) in enumerate(train_examples):
    #     print("id_a:{}\ttext_a: {}".format(example.id_a, example.text_a))
    #     print("id_b:{}\ttext_b: {}".format(example.id_b, example.text_b))
    # split samples into training and validation set
    from sklearn.model_selection import train_test_split
    train_examples, validation_examples = train_test_split(train_examples, test_size=0.2, shuffle=True)



    from keras.models import Sequential
    from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Dense, LSTM, Bidirectional
    from keras.regularizers import l1, l1_l2, l2
    #   tf.reset_default_graph()
    #   from keras import backend as K
    #   K.clear_session()

    ###################################### work ############################################
    #   model = tf.keras.Sequential([
    #         tf.keras.layers.Conv1D(filters=256, kernel_size=15, strides=1, padding='same', activation='relu', input_shape=(1024, 3)),
    #         tf.keras.layers.MaxPooling1D(),
    #         tf.keras.layers.Conv1D(filters=128, kernel_size=10, strides=1, padding='same', activation='relu'),
    #         tf.keras.layers.MaxPooling1D(),
    #         tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu'),
    #         tf.keras.layers.MaxPooling1D(),
    #         tf.keras.layers.Flatten(),
    #         tf.keras.layers.Dropout(0.1),
    #         tf.keras.layers.Dense(64, activation='relu'),
    #         tf.keras.layers.BatchNormalization(),
    #         tf.keras.layers.Dense(2, activation='softmax')
    #     ])


    ##########################################################################################

    model = Sequential([
        Conv1D(filters=256, kernel_size=15, strides=1, padding='same', activation='relu', input_shape=(1024, 3)),
        MaxPooling1D(),
        Conv1D(filters=128, kernel_size=10, strides=1, padding='same', activation='relu', kernel_regularizer=l2(0.001)),
        MaxPooling1D(),
    #         tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu'),
    #         tf.keras.layers.MaxPooling1D(),
        Bidirectional(LSTM(32, return_sequences=True, recurrent_regularizer=l2(0.01))),
        Flatten(),
        Dropout(0.1),
        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
        BatchNormalization(),
        Dense(1, activation='sigmoid')
    ])

    # Model is the full model w/o custom layers

    from keras.callbacks import TensorBoard

    tensorboard = TensorBoard(log_dir='logs', histogram_freq=0,
                          write_graph=True, write_images=True)

    model.compile(optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy'])

    metrics_names = model.metrics_names
    model.summary()
    #   train_generator = get_batches_elmo_multi(train_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size)
    #   model.fit_generator(train_generator, steps_per_epoch=10, epochs=10)
    #   eval_generator = get_batches_elmo_multi(validation_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size)
    #   model.evaluate_generator(eval_generator, steps_per_epoch=10, epochs=10)

    training_start_time = time.time()

    history = model.fit_generator(generator=get_batches_elmo_multi_2(train_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size),
                                  steps_per_epoch=len(train_examples) // FLAGS.batch_size,
                                  validation_data=get_batches_elmo_multi_2(validation_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size),
                                  validation_steps=len(validation_examples) // FLAGS.batch_size,
                                  epochs=FLAGS.epochs,
                                  # use_multiprocessing=True,
                                  callbacks=[tensorboard])
    training_duration = time.time() - training_start_time
    print("Total training time: {}".format(training_duration))
    print(history.history.keys())
    print("Train: ", "acc: {} \t loss: {}".format(history.history['acc'], history.history['loss']))

    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    # summarize history for accuracy
    plt.figure()
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.savefig(FLAGS.img_path + FLAGS.img_name + 'training_loss.png')
    plt.show()
    # summarize history for loss
    plt.figure()
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.savefig(FLAGS.img_path + FLAGS.img_name + 'training_accuracy.png')
    plt.show()


    eval_history = model.evaluate_generator(
        get_batches_elmo_multi(eval_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size),
        steps=len(eval_examples)//FLAGS.batch_size)

    print("Evaluate: ", "acc: {} \t loss: {}".format(eval_history[1], eval_history[0]))

    # predict_history = model.evaluate_generator(
    #     get_batches_elmo_multi(predict_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size),
    #     steps=len(predict_examples) // FLAGS.batch_size)
    #
    # print("Predict: ", "acc: {} \t loss: {}".format(predict_history['acc'], predict_history['loss']))

    predict_labels = []
    true_labels = []
    for x, y in get_batches_elmo_multi(predict_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size):
      result = model.predict(x)
    #       print(result[1])
      result = np.where(result > 0.5, 1, 0)
      true_labels.extend(y)
      predict_labels.extend(result)
    #   print("predict: ", "average {}: {:.6f}".format(metrics_names[1], np.mean(predict_labels)))
    from sklearn.metrics import classification_report
    print(classification_report(true_labels, predict_labels))

    ####################################################
    iteration = 0
    train_acc = []
    train_loss = []
    validation_acc = []
    validation_loss = []
    # for epoch in range(FLAGS.epochs):
    #     #Reset the metric accumulators
    #     # model.reset_metrics()
    #     # Loop over batches
    #
    #     for x, y in get_batches_elmo_multi(train_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size):
    #         result = model.train_on_batch(x, y)
    #         loss = result[0]
    #         acc = result[1]
    #         train_acc.append(acc)
    #         train_loss.append(loss)
    #         if (iteration % 50 == 0):
    #             print("Epoch: {}/{}".format(epoch, FLAGS.epochs),
    #                   "Iteration: {:d}".format(iteration),
    #                   "train: ",
    #                   "{}: {:.3f}".format(metrics_names[0], result[0]),
    #                   "{}: {:.3f}".format(metrics_names[1], result[1]))
    #
    #         if (iteration % 100 == 0):
    #             val_acc_ = []
    #             val_loss_ = []
    #             for x, y in get_batches_elmo_multi(validation_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size):
    #                 result = model.test_on_batch(x, y)
    #                 loss_v = result[0]
    #                 acc_v = result[1]
    #                 val_acc_.append(acc_v)
    #                 val_loss_.append(loss_v)
    #
    #             print("Epoch: {}/{}".format(epoch, FLAGS.epochs),
    #                   "Iteration: {:d}".format(iteration),
    #                   "eval: ",
    #                   "{}: {:.3f}".format(metrics_names[0], result[0]),
    #                   "{}: {:.3f}".format(metrics_names[1], result[1]))
    #             validation_acc.append(np.mean(val_acc_))
    #             validation_loss.append(np.mean(val_loss_))
    #         # Iterate
    #         iteration += 1

    # eval_acc = []
    # for x, y in get_batches_elmo_multi(eval_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size):
    #     result = model.evaluate(x, y)
    #     batch_acc = result[1]
    #     eval_acc.append(batch_acc)
    # print("evaluate: ", "average {}: {:.6f}".format(metrics_names[1], np.mean(eval_acc)))
    #
    # predict_acc = []
    # for x, y in get_batches_elmo_multi(predict_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size):
    #     result = model.evaluate(x, y)
    #     batch_acc = result[1]
    #     predict_acc.append(batch_acc)
    # print("predict: ", "average {}: {:.6f}".format(metrics_names[1], np.mean(predict_acc)))
    #
    # predict_labels = []
    # true_labels =[]
    # for x, y in get_batches_elmo_multi(predict_examples, embed=embedding_result, embed_2=embedding_result_2, batch_size=FLAGS.batch_size):
    #   result = model.predict(x)
    # #       print(result[1])
    #   result = result.argmax(axis=-1)
    #   true_labels.extend(y)
    #   predict_labels.extend(result)
    # #   print("predict: ", "average {}: {:.6f}".format(metrics_names[1], np.mean(predict_labels)))
    # from sklearn.metrics import classification_report
    # print(classification_report(true_labels, predict_labels))
    ###################################################################################################

    model.save('my_model_20.hdf5')

if __name__ == "__main__":
    # flags.mark_flag_as_required("data_dir")
    # flags.mark_flag_as_required("vocab_file")
    # flags.mark_flag_as_required("output_dir")

    FLAGS.directory_path = "SNO/"
    FLAGS.iterations = 40
    FLAGS.seq_len = 1024
    FLAGS.n_channels = 3
    FLAGS.hier_name = "procedure"
    FLAGS.taxonomy = "hier"
    FLAGS.img_path = FLAGS.directory_path + "img/"
    FLAGS.img_name = timestamp + FLAGS.hier_name + "_" + FLAGS.taxonomy + "_iter_" + str(FLAGS.iterations) + "_"
    FLAGS.vocab_file = "MODEL/small/vocab.txt"
    FLAGS.batch_size = 500
    FLAGS.epochs = 20  # 10
    # FLAGS.data_dir = "../bert/glue_data/MYTESTING/Iteration_1/"
    FLAGS.data_dir = "SNO/TestData/Iteration_1/"
    FLAGS.learning_rate = 0.0001
    FLAGS.output_dir = "cnnModel/elmo/Iteration_1/"

    # FLAGS.init_checkpoint = "tmp/procedure/pretraining_output/model.ckpt-10000"  # load the pre-trained model


    FLAGS.do_train = True  # True
    FLAGS.do_eval = True  # True
    FLAGS.do_predict = True  # False
    FLAGS.task_name = "CNN"

    tf.app.run()